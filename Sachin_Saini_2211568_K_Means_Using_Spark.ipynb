{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhnFmxLxOWEr",
        "outputId": "07b49d2c-e17a-4e4a-f6e3-08eda795da0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n",
            "openjdk-8-jdk-headless is already the newest version (8u342-b07-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#Setuping the spark\n",
        "!pip install pyspark #installing pyspark \n",
        "!pip install -U -q PyDrive ##PyDrive is a wrapper library of google-api-python-client that simplifies many common Google Drive API tasks.\n",
        "!apt install openjdk-8-jdk-headless -qq # #software capable of working on a device without a graphical user interface. Such software receives inputs and provides output through other interfaces like network or serial port and is common on servers and embedded devices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" #The path is the most important environment variable of the Java environment which is used to locate the JDK packages that are used to convert the java source code into the machine-readable binary format\n"
      ],
      "metadata": {
        "id": "S7KWW2MeOtee"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #library for dataframes\n",
        "import numpy as np #library for arrays\n",
        "import matplotlib.pyplot as plt # library for visualization\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark #library for processing big data\n",
        "from pyspark.sql import * # this is library for sql data processing.\n",
        "from pyspark.sql.types import * #importing the datatypes of sql using spark.\n",
        "from pyspark.sql.functions import * #importing all sql functions\n",
        "from pyspark import SparkContext, SparkConf # for creating spark sessiong and configuring it so that transformation and actions can be applied to RDDs."
      ],
      "metadata": {
        "id": "spI4V52hPV0Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Creating Spark Session\n",
        " conf=SparkConf().set(\"Spark.ui.port\",\"4050\")\n",
        "\n",
        " #creat the context\n",
        " sc=pyspark.SparkContext(conf=conf)\n",
        " spark=SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "zCsHHawPQiO9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the version \n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "8yrlBGQYRK_M",
        "outputId": "c4bb07a4-fc5a-4ab6-9113-d1fa1233d8b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2fc1f5acd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f2e82ddeda48:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If Google colab is running on the google hosted runtime,then the cell below will create a ngrok tunnel which allow us to check spark UI."
      ],
      "metadata": {
        "id": "Qbc6pOfThuH_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gsHwKHziiEGB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys,json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic0C7TYJRXa8",
        "outputId": "c829bb6c-e806-4c50-d697-51dc175b7f0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-22 10:43:39--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.4’\n",
            "\n",
            "\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \rngrok-stable-linux- 100%[===================>]  13.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-10-22 10:43:39 (103 MB/s) - ‘ngrok-stable-linux-amd64.zip.4’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/lib/python3.7/json/__init__.py\", line 296, in load\n",
            "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
            "  File \"/usr/lib/python3.7/json/__init__.py\", line 348, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.7/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.7/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n"
      ],
      "metadata": {
        "id": "uqsEMZakjXll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading breast cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "metadata": {
        "id": "VvVP4TH_i3cT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GnlbuqGkkCtt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this dataset is small, so we will first make a dataframe and then convert it into Spark Dataframe."
      ],
      "metadata": {
        "id": "5XxEJh90juzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gyq5m8CXkC9L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df=pd.DataFrame(breast_cancer.data,columns=breast_cancer.feature_names)\n",
        "df=spark.createDataFrame(pd_df)\n",
        "def set_df_columns_nullable(spark,df,column_list,nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable= nullable\n",
        "    df_mod=spark.createDataFrame(df.rdd,df.schema)\n",
        "    return df_mod\n",
        "\n",
        "df=set_df_columns_nullable(spark,df,df.columns)\n",
        "df=df.withColumn('features',array(df.columns))\n",
        "vectors=df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "\n",
        "df.printSchema() # printing the schema of breast cancer dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97AkznxmjmsW",
        "outputId": "f177a56d-622b-4dcf-e478-e2415986ba41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now We will make two datastructure that we will use throught this colab:\n",
        "\n",
        "> Features- A dataframe of dense vector,containing all the orignal features in the dataset.\n",
        "\n",
        "> Labels: A series of binary labels indicating if the corresponding set of features belongs to subject with breast cncer or not. "
      ],
      "metadata": {
        "id": "kLWO0DRLoCLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors # importing the vectors of pyspark it is similar to numpy arrays.\n",
        "features=spark.createDataFrame(vectors.map(Row),[\"features\"])\n",
        "labels=pd.Series(breast_cancer.target)"
      ],
      "metadata": {
        "id": "l3gbUD5JkWG2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5rKJ5GUluiP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Kmeans Clustering model using Spark\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Trains a k-means Model\n",
        "\n",
        "kmeans=KMeans().setK(2).setSeed(1)\n",
        "model=kmeans.fit(features)\n",
        "\n",
        "#Making Predictions\n",
        "\n",
        "predictions=model.transform(features)\n",
        "\n",
        "#Evaluation model using Silhouette score\n",
        "evaluator= ClusteringEvaluator()\n",
        "\n",
        "Silhouette=evaluator.evaluate(predictions) # Finding score using silhouette\n",
        "print(\"Silhoutee with scored euclidian distance =\"+ str(Silhouette))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh4cgdaOq0_a",
        "outputId": "3a38f3dc-4983-416c-8b3e-73f432246e66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhoutee with scored euclidian distance =0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_series=predictions.select(\"prediction\").toPandas()\n",
        "true_prediction_count=np.count_nonzero(predictions_series['prediction']==labels)\n",
        "if true_prediction_count<len(labels)-true_prediction_count:\n",
        "  true_prediction_count=len(labels)-true_prediction_count\n",
        "print('precison:',true_prediction_count/len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIjHPQTL0rZC",
        "outputId": "e043cc73-2b2c-4669-cfd7-ef4c9c5a945d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precison: 0.8541300527240774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAGy3-6B4HZJ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}